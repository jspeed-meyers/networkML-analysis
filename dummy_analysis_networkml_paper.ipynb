{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, \\\n",
    "    roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import session dataframe\n",
    "file_location = \"/Users/jmeyers/Desktop/networkML/paper/data/dummy_data/dummy_session.csv.gz\"\n",
    "session_df = pd.read_csv(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Y\n",
    "y = session_df.filename # identify filename column\n",
    "y = y.str.split('-')\n",
    "y = y.str[0] # device type is always first field\n",
    "\n",
    "# !! DANGER !!\n",
    "# CODE MUST BE REMOVED FOR REAL ANALYSIS\n",
    "# Making first thousand rows a printer ensures\n",
    "# that all cross validation runs have more than\n",
    "# two target values\n",
    "y[0:500] = \"printer\"\n",
    "y[501:1000] = \"phone\"\n",
    "# !! DANGER !!\n",
    "\n",
    "# Convert y into categorical variable\n",
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "y = le.transform(y)\n",
    "\n",
    "# Extract X\n",
    "# Drop filename and host_key columns\n",
    "X = session_df.drop(columns=['filename', 'host_key'])\n",
    "\n",
    "# Split into train and test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=20200313)\n",
    "\n",
    "# Normalize data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assessModel(model, parameters):\n",
    "    \"\"\"Assess model performance given model type and paramters\n",
    "    \n",
    "    INPUT:\n",
    "    --model: type of machine learning model (sklearn models)\n",
    "    --parameters: Dictionary of parameters and values to do grid search\n",
    "    \n",
    "    OUTPUT:\n",
    "    --prints performance metrics (accuracy, precision, recall, F1s)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Do 5-fold cross validation with training data\n",
    "    # Combine with hyper-parameter optimization to do a simple\n",
    "    # but principled search\n",
    "    # TODO: SET CV=5 WHEN DOING ACTUAL EXPERIMENT\n",
    "    clf = GridSearchCV(model, parameters, cv=3, n_jobs=-1, scoring='f1_weighted')\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get best performing weight model for each model class\n",
    "    # Predict on test set\n",
    "    best_mod = clf.best_estimator_\n",
    "    y_pred = best_mod.predict(X_test_scaled)\n",
    "    y_pred_prob = best_mod.predict_proba(X_test_scaled)\n",
    "\n",
    "    ## Calculate metrics\n",
    "    acc  = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted')\n",
    "    rec  = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1   = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print([acc, prec, rec, f1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9529001308329699, 0.9237765904044453, 0.9529001308329699, 0.9316660540641136]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jmeyers/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Test with logistic regression for simplicity\n",
    "parameters = {'C':[1, 2]}\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "assessModel(model, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9993458351504579, 0.9993517463861991, 0.9993458351504579, 0.9993477630599079]\n"
     ]
    }
   ],
   "source": [
    "# Test with decision tree for next level of complexity\n",
    "parameters = {'max_depth':[4, 5]}\n",
    "model = tree.DecisionTreeClassifier(random_state = 1865)\n",
    "assessModel(model, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9550806803314436, 0.9571041616023209, 0.9550806803314436, 0.9354503259417334]\n"
     ]
    }
   ],
   "source": [
    "# Test with random forests for next level of complexity\n",
    "parameters = {'max_depth':[2,3]}\n",
    "model = RandomForestClassifier(max_depth=2, random_state=55)\n",
    "assessModel(model, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9529001308329699, 0.9340380562554633, 0.9529001308329699, 0.9381854118703289]\n"
     ]
    }
   ],
   "source": [
    "# Test with KNN for next level of complexity\n",
    "parameters = {'n_neighbors':[3,4]}\n",
    "model = KNeighborsClassifier()\n",
    "assessModel(model, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe consider XGboost too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9520279110335804, 0.9312333647150078, 0.9520279110335804, 0.931981581284562]\n"
     ]
    }
   ],
   "source": [
    "# Test with neural network from sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "parameters = {'hidden_layer_sizes':[(64,32), (32,16)]}\n",
    "model = MLPClassifier()\n",
    "assessModel(model, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try models from model-list\n",
    "# - KNN\n",
    "# - Decision tree\n",
    "# - Random forest\n",
    "# - Neural network (Do I need to use Keras/tensorflow? Maybe)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
